{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1주차 신경망의 수학적 구성요소\n",
    "1. 간단한 사용\n",
    "2. 데이터 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 간단한 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0708 16:49:41.663896 22248 deprecation_wrapper.py:119] From D:\\Anaconda3\\envs\\kera\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0708 16:49:41.688910 22248 deprecation_wrapper.py:119] From D:\\Anaconda3\\envs\\kera\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0708 16:49:41.692808 22248 deprecation_wrapper.py:119] From D:\\Anaconda3\\envs\\kera\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras import models, layers\n",
    "from keras.utils import to_categorical\n",
    "(train_img, train_labels),(test_img, test_labels)=mnist.load_data()\n",
    "print(test_img.shape)\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512,activation='relu',input_shape=(28*28,)))\n",
    "network.add(layers.Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥러닝은 간단한층 여러개를 연결하여 점진적으로 정제  \n",
    "코드 설명\n",
    "   >층 2개로 구성, 2번층 : 10개의 확률점수가 들어있는 배열을 반환하는 소프트 맥스 층  \n",
    "   >각 점수는 숫자 이미지가 10개의 숫자 클래스중 하나에 속할 확률"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신경망이 훈련 준비를 마치기 위한 컴파일 단계의 준비물\n",
    "***\n",
    "- 손실함수\n",
    "- 옵티마이저\n",
    "- 훈련과 테스트 과정을 모니터링할 지표\n",
    "![pre](img1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0708 16:49:45.742379 22248 deprecation_wrapper.py:119] From D:\\Anaconda3\\envs\\kera\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0708 16:49:45.781104 22248 deprecation_wrapper.py:119] From D:\\Anaconda3\\envs\\kera\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "network.compile(optimizer = 'rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "train_img = train_img.reshape((60000, 28*28))\n",
    "train_img = train_img.astype('float32')/255\n",
    "test_img = test_img.reshape((10000,28*28))\n",
    "test_img = test_img.astype('float32')/255\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터를 네트워크에 맞는 크기(28 * 28)로 바꾸고  \n",
    "모든 값을 0 과 1 사이로 조정  \n",
    "train_img 가 0과 1사 이의 (60000,28*28)크기의 배열로 바뀜  \n",
    "레이블을 범주형으로 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0708 16:49:49.955240 22248 deprecation.py:323] From D:\\Anaconda3\\envs\\kera\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0708 16:49:50.202106 22248 deprecation_wrapper.py:119] From D:\\Anaconda3\\envs\\kera\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.2575 - acc: 0.9245\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.1031 - acc: 0.9691\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.0680 - acc: 0.9798\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 0.0497 - acc: 0.9849\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 0.0379 - acc: 0.9883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x225e9588860>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(train_img, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 55us/step\n",
      "test_acc: 0.9796\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = network.evaluate(test_img, test_labels)\n",
    "print('test_acc:',test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 신경망의 데이터 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 핵심속성\n",
    "1.랭크(축) 2.크기 3.데이터타입\n",
    "\n",
    "1. 랭크 : 데이터의 차원수 .ndim의 속성\n",
    "> 0D 텐서 1개의 숫자만 : 스칼라    : array(3)  \n",
    "> 1D 텐서 1개 이상의 숫자(1줄) : 벡터     : array( [12, 3, 6, 14, 7])  \n",
    "> 2D 텐서 2차원        : 행렬     array([[12,3,5],[2,3,4]])  \n",
    "> ND 텐서 N개의 차원을 가진 텐서  \n",
    "\n",
    "2. 크기 .shape의 속성\n",
    "> 크기 : 텐서의 각 축을 따라 얼마나 많은 차원이 있는지를 나타낸 데이터  \n",
    "행렬의 크기 : (2,5)  \n",
    "벡터의 크기 : (5,)  \n",
    "스칼라의 크기 : () 없음\n",
    "\n",
    "3. 데이터 타입\n",
    ">케라스, 탠서플로우는 데이터 구조로 주로 numpy라이브러리를 활용 >> 속도향상 목적   \n",
    "<small> impot numby </small>\n",
    "주로 float32, unit8, float64 등의 데이터 타입을 사용\n",
    "\n",
    "\n",
    "* array    (numpy.array)\n",
    "* int, uint(8, 16, 32, 64)\n",
    "* float, complex(16, 32, 64, 128)\n",
    "* bool, object, string_, unicode_  \n",
    "등의 데이터 타입이 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### numpy 연산자\n",
    "1. numpy.array()  array 생성  #array([1,2,3,4])\n",
    "2. numpy.zeros(size) size크기의 0으로 이뤄진 array형 데이터 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 배치\n",
    "주로 데이터의 0번째 축 은 샘플축  \n",
    "한번에 전체 데이터셋을 처리하는것은 불가능 (메모리의 용량, 효율성 등의 이유)  \n",
    " > 데이터를 작은 배치로 나눠서 처리  \n",
    " > batch = train_img[128*n : 128*(1+n)]     128크기의 배치를 이용할시 n번째 배치  \n",
    " \n",
    "배치 데이터 사용시 0번째 축을 배치축 또는 배치차원이라 함 (it calls as batch axis or batch dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 텐서의 형태\n",
    "1. 벡터 데이터 : (sanples, features) 크기의 2D텐서\n",
    "> 나이, 우편번호, 소득으로 구성된 데이터, 각 사람은 3개의 값을 가진 벡터로 구성 해당 데이터셋은 (사람수, 3)크기의 텐서에 저장 가능\n",
    "> 샘플축 크기 : 사람수 , 특성축 크기: 3\n",
    "2. 시계열 데이터 또는 시퀀스 데이터 : (samples, timesteps, features)크기의 3D텐서\n",
    "> 시계열에서의 샘플이 x1, ..., xn까지 있다고 할 때, 각각의 샘플들은 특정한 시간 t에서 측정된 데이터  \n",
    "> 식물의 키 대한 데이터 : 3개의 화분, 365일, 키에 대한 테이터  \n",
    "> (3,365,1)의 3D 텐서에 저장가능, 단 관례적으로 시간축으 항상 1번째 축\n",
    "3. 이미지 : (samples, height, width, channels) 크기의 4D텐서\n",
    "> 256*256 크기 이미지 128의 배치 (128, 256, 256, 3)크기의 텐서에 저장  - rgb체널로 컬러 체널의 크기 : 3\n",
    "4. 동영상   (samples, framse, height, width, channels)크기의 5D텐서  \n",
    "> 60초짜리 144%256 영상을 초당 4프레임으로 셈플링 ->프레임수 : 240\n",
    "> 해당 셈플 4개 (4,240,144,256,3)크기의 텐서에 저장가능\n",
    "[^TimeSerial]: ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 텐서연산\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import numpy as np\n",
    "keras.layers.Dense(512,activation='relu')\n",
    "output=relu(dot(W,inpuy)+b)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 원소별 연산 : + - * /\n",
    "2. 행렬합\n",
    "    1. 브로드 캐스팅 : 크기가 서로 다른 두 텐서를 더할때 행해짐  (자동)\n",
    "    큰 텐서의 .ndim값과 동일해지게 작은 텐서의 축이 추가됨(브로드 캐스팅 축)  \n",
    "    작은 텐서가 새 축을 따라 큰 텐서의 크기에 맞게 반복됨\n",
    "    > (32,10)크기 x 와 (10,)크기 y의 합  \n",
    "    y가 (1,10)으로 변형 (브로드 캐스팅 축 추가)\n",
    "    y가 (32,10)으로 변형 (1,10)32회 반복\n",
    "    \n",
    "3. 텐서 점곱\n",
    "    keras.dot(x,y) : 내적곱  \n",
    "    keras.matul(x,y) or numpy.matul(x,y) or x@y : 행렬곱  \n",
    "    2차원 초과시 결과에 차이가 남, 두 차이는 알아봐야함\n",
    "4. 텐서 크기 변환  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]]\n",
      "(6, 1)\n",
      "[[0]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x=np.array([[0,1],[2,3],[4,5]])\n",
    "print(x.shape)\n",
    "print(x)\n",
    "x=x.reshape((6,1))\n",
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 신경망의 엔진\n",
    "#### 1. 그래디언트 기반 최적화\n",
    "각 층은 다음과 같은 형태로 생성됨  \n",
    "keras.layers.Dense(512,activation='relu')  \n",
    "각 층에서 입력 데이터를 다음과 같이 변환함  \n",
    "**output=relu(dot(W,input)+b)** (W: 2D, b : 1D텐서, relu : 0보다 크면 해당값, 아니면 0 을 반환)  \n",
    "w : **가중치(weight)**,커널(kernel)  \n",
    "b : 훈련되는 파라미터(trainable parameter), **편향(bias)** 라고 불림  \n",
    "초기에는 w, b가 난수로 이뤄져 있음\n",
    "피드백 신호에 기초하여 가중치가 점진적으로 조정, 이런과정을 **훈련**이라고 함  \n",
    "훈련은 **훈련 반복** 안에서 일어남\n",
    "1. 훈련 샘플x와 이에 상응하는 타깃y의 배치를 추출\n",
    "2. x를 사용하여 네트워크 실행, 예측 y_pred를 산출\n",
    "3. y_pred와 y의 차를 축정하여 이 배치에 대한 네트워크 손실계산\n",
    "4. 배치에 대한 손실이 감소 하도록 네트워크의 모든 가중치를 업데이트  \n",
    "과정 모식도\n",
    "![pre](img1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가중치를 업데이트 하는과정이 어려움  \n",
    "초기 가중치가 0.3, 정방향 패스 통가후 손실 0.5 > 가중치 0.35로 변경후 재실행 : 손실 0.6 > 가중치 0.25 변경후 재실행 : 손실 0.4  \n",
    "해당 과정은 모든 가중치 행렬에 대하여 2번이상의 정방향 패스를 계산해야하므로 상당히 비효율적  \n",
    "-> 신경망에 사용된 연산이 미분가능 -> 네트워크 가중치에 대한 손실의 그래디언트를 계산, 그래디언트의 반대방향으로 이동 -> 손실감소\n",
    "#### 2. 그래디언트란\n",
    "텐서 연산의 변화율\n",
    "-> 공간에 대한 변화률\n",
    "\n",
    "$gradf=\\nabla f=\\frac{\\partial f}{\\partial x}i+\\frac{\\partial f}{\\partial y}j+\\frac{\\partial f}{\\partial z}k=\\left[\\frac{\\partial f}{\\partial x},\\frac{\\partial f}{\\partial y},\\frac{\\partial f}{\\partial z}\\right]$  \n",
    "미분연산자만을\\ 고려하면\n",
    "$ \\nabla =\\frac{\\partial }{\\partial x}i+\\frac{\\partial }{\\partial y}j+\\frac{\\partial }{\\partial z}k $  \n",
    "예시) $f(t)=5xy+y^2+2z$  \n",
    "$ \\frac{\\partial}{\\partial x} = 5x $, $\\frac{\\partial }{\\partial y}=5x+2y$, $\\frac{\\partial }{\\partial z}=2$  \n",
    "$graf = \\frac{\\partial f}{\\partial x}i+\\frac{\\partial f}{\\partial y}j+\\frac{\\partial f}{\\partial z}k+[5y, 5x+2y, 2]$  \n",
    "\n",
    "y_pred$=g(w,x)$  \n",
    "loss_value $= loss($y_pred$,y)$  \n",
    "x, y가 고정되어있으면 w에 대한 손실함수 f(w)로 볼수있음  \n",
    "이때 w에서의 f(w)의 변화율은 w와 같은 크기의 텐서인 gradient(w)\n",
    "#### 3. 경사 하강법\n",
    "\n",
    "> 1. batch gradient Descent - 전채 트래이닝 데이터를 하나의 batch 로 사용 (모든손실함수에 미분값 대한 편균을 취하여 파라메터를 업데이트)  \n",
    "> 2. 확률적 경사 하강법 (stochastic gradient descent)<big>SGD</big> 1개의 트레이닝 데이터만 사용  \n",
    "> 3. 미니 배치 확률적 경사 하강법 (mini-batch stochastic gradient Descent)<big>미니배치 SGD</big> 미니배치중에 확률적으로 선택\n",
    "\n",
    "\n",
    "1. 훈련셈플 배치 x 와 상응하는 y를 추출\n",
    "2. x로 네트워크 실행, 예측y_pred\n",
    "3. y, y_pred의 오차를 측정하여 네트워크의 손실 계산\n",
    "4. 세트워크의 파라미터에 대한 손실함수의 그래디언트를 계산\n",
    "5. 그래디언트의 역방향으로 한스텝한스텝이동 (W = w- step*gradient)\n",
    "\n",
    "그래디언트의 방향으로 한스텝한스텝 이동\n",
    "스텝에 대한 조정 필요(한번에 이동하는량)\n",
    "\n",
    "##### 모멘텀\n",
    "![grobal Minal](https://postfiles.pstatic.net/20151209_169/2011topcit_1449656256921axqqx_GIF/%B5%F6%B7%AF%B4%D7%C0%C7_%C0%CC%C7%D8_%B0%ED%B1%DE%B0%E6%BB%E7%C7%CF%B0%AD%B9%FD1.gif?type=w2)\n",
    "가속도를 활용하여 지역 최소를 지나갈수 있음\n",
    "\n",
    "#### 4. 역전파 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 기초이론\n",
    "\n",
    "#### 1. 오버피팅\n",
    "![fitting](https://mblogthumb-phinf.pstatic.net/MjAxNzAzMjdfMTkx/MDAxNDkwNjI1NDA1MTE2.uQ-iyqt__xd3FDhLRg_agENYiyThSfTkSiDZq9RqHeAg.K8bnqBMCgBL3dD9QA79TcW3Lw82glQFVAgU7urzKG1Eg.PNG.samsjang/%EC%BA%A1%EC%B2%982.PNG?type=w2)\n",
    "keyord : regularization\n",
    "#### 2. 소프트맥스\n",
    "Softmax(소프트맥스)는 출력으로 0~1사이의 값으로 가지며 출력 값들의 총합은 항상 1이 되는 특성을 가진 함수.  \n",
    "$sotfmax(x)_i = \\frac{exp(x_i)}{\\Sigma_j{exp(x_j)}}$  \n",
    "[0,7, 0,3, 0,2]\n",
    "#### 3. 원 핫 인코딩\n",
    "개 = 1, 고양이= 2 토끼 = 3  \n",
    "토끼 = 개 + 고양이  \n",
    "개 = [1 0 0] 고양이 = [0 1 0] 토끼 = [0 0 1]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 신경망의 구조\n",
    "#### 1. 구조\n",
    "> 층 : 네트워크 모델을 구성  \n",
    "입력데이터와 상응하는 타깃  \n",
    "손실함수  \n",
    "옵티마이저\n",
    "\n",
    "#### 2. 층\n",
    "1. 층 : 1이상의 텐서를 입력받아 1이상의 텐서를 출력하는 데이터 처리 모듈  \n",
    "    대부분 가중치 라는 값을 가짐.  \n",
    "2. 가중치 : 확률적 경사 하강법에 의해 학습되는 1이상의 텐서, 네트워크가 학습한 지식을 가짐  \n",
    "\n",
    "#### 3. 모델\n",
    "딥러닝 모델은 층으로 만든 Directed Acycle Graph(DAG)형태.\n",
    "예시로 1의 입력을 1의 출력으로 맵핑하는 층을 순서대로 쌓는것\n",
    "#### 4. 손실함수와 옵티마이저\n",
    "여러개의 출력을 만드는 신경망은 여러개의 손실함수가 있을수 있음.\n",
    "경사하강법은 하나의 스칼라 손실값을 기준으로 함 -> 모든 손실값 평균을 내서 하나의 스칼라로 만들음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
